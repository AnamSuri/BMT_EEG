{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Train data from first 2 sessions\n",
    "    -Genuine class (1)\n",
    "    samples of VEP class of user 1\n",
    "    -Forged class (0)\n",
    "    samples of non-VEP of users other than user 1\n",
    "    i.e. remaining 19 users (other than user 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Test data from third session:\n",
    "    -Genuine class (1)\n",
    "    samples of VEP class of user 1\n",
    "    -Forged class (0)\n",
    "    samples of non-VEP of all other users i.e. remaining 19 users (other than user 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_subject = 'S20'\n",
    "\n",
    "all_subjects = ['S1', 'S2', 'S3', 'S4', 'S5', 'S6','S7', 'S8', 'S9', 'S10','S11', 'S12', 'S13', 'S14', 'S15', 'S16','S17', 'S18', 'S19', 'S20']\n",
    "\n",
    "tasks_non_vep = ['REO', 'MI1', 'MM1', 'MI2', 'MM2', 'MI3', 'MM3', 'MI4', 'REC']\n",
    "\n",
    "training_sessions = ['S1', 'S2']\n",
    "\n",
    "test_session = ['S3']\n",
    "\n",
    "path = 'PATH_TO_DATA'\n",
    "\n",
    "lookback = 100\n",
    "\n",
    "#batch_size = 32\n",
    "batch_size = 32\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "input_channels = 20\n",
    "\n",
    "sequence_length = 100\n",
    "\n",
    "#hidden_size = 128\n",
    "hidden_size = 200\n",
    "\n",
    "#num_epochs = 40\n",
    "num_epochs = 60\n",
    "\n",
    "num_folds = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to create dataset\n",
    "def create_dataset(df, lookback, label):\n",
    "    df = df.to_numpy()\n",
    "    X, y = [], []\n",
    "    for i in range(0, len(df) - lookback + 1, lookback):\n",
    "        X.append(df[i:i + lookback])\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For genuine user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_VEP = pd.DataFrame()\n",
    "for session in training_sessions:\n",
    "    vep_path = os.path.join(path, target_subject, 'VEP3', f'{session}.csv' )\n",
    "    vep_data = pd.read_csv(vep_path)\n",
    "    Data_VEP = pd.concat([Data_VEP, vep_data], ignore_index=True)\n",
    "\n",
    "Data_VEP.drop(['time'], axis=1, inplace=True)\n",
    "normalize_vep = normalize(Data_VEP, norm='max', axis=0)\n",
    "Data_VEP = pd.DataFrame(normalize_vep, columns=Data_VEP.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56320, 20)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_VEP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56320"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_vep_samples_train = len(Data_VEP)\n",
    "num_vep_samples_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Forged user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_non_VEP = pd.DataFrame()\n",
    "for sub in all_subjects:\n",
    "    if sub == target_subject:\n",
    "        continue\n",
    "    for task in tasks_non_vep:\n",
    "        for session in training_sessions:\n",
    "            non_vep_path = os.path.join(path, sub, task, f'{session}.csv')\n",
    "            non_vep_data = pd.read_csv(non_vep_path)\n",
    "            Data_non_VEP = pd.concat([Data_non_VEP, non_vep_data], ignore_index=True)\n",
    "\n",
    "Data_non_VEP.drop(['time'], axis=1, inplace=True)\n",
    "normalize_non_vep = normalize(Data_non_VEP, norm='max', axis=0)\n",
    "Data_non_VEP = pd.DataFrame(normalize_non_vep, columns=Data_non_VEP.columns)\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_non_VEP_sampled = Data_non_VEP.sample(n=num_vep_samples_train, random_state=987)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vep_train, y_vep_train = create_dataset(Data_VEP, lookback, label=1)\n",
    "X_non_vep_train, y_non_vep_train = create_dataset(Data_non_VEP_sampled, lookback, label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((563, 100, 20), (563,), (563, 100, 20), (563,))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_vep_train.shape, y_vep_train.shape, X_non_vep_train.shape, y_non_vep_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_non_vep_train, X_vep_train), axis=0)\n",
    "y_train = np.concatenate((y_non_vep_train, y_vep_train), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1126, 100, 20), (1126,))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Genuine User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_VEP_test = pd.DataFrame()\n",
    "for session in test_session:\n",
    "    vep_path = os.path.join(path, target_subject, 'VEP3', f'{session}.csv' )\n",
    "    vep_data_test = pd.read_csv(vep_path)\n",
    "    Data_VEP_test = pd.concat([Data_VEP_test, vep_data_test], ignore_index=True)\n",
    "\n",
    "Data_VEP_test.drop(['time'], axis=1, inplace=True)\n",
    "normalize_vep_test = normalize(Data_VEP_test, norm='max', axis=0)\n",
    "Data_VEP_test = pd.DataFrame(normalize_vep_test, columns=Data_VEP_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28160, 20)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_VEP_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28160"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_vep_samples_test = len(Data_VEP_test)\n",
    "num_vep_samples_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Forged user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_non_VEP_test = pd.DataFrame()\n",
    "for sub in all_subjects:\n",
    "    if sub == target_subject:\n",
    "        continue\n",
    "    for task in tasks_non_vep:\n",
    "        for session in test_session:\n",
    "            non_vep_path = os.path.join(path, sub, task, f'{session}.csv')\n",
    "            non_vep_data_test = pd.read_csv(non_vep_path)\n",
    "            Data_non_VEP_test = pd.concat([Data_non_VEP_test, non_vep_data_test], ignore_index=True)\n",
    "\n",
    "Data_non_VEP_test.drop(['time'], axis=1, inplace=True)\n",
    "normalize_non_vep_test = normalize(Data_non_VEP_test, norm='max', axis=0)\n",
    "Data_non_VEP_test = pd.DataFrame(normalize_non_vep_test, columns=Data_non_VEP_test.columns)\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2188800, 20)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_non_VEP_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data_non_VEP_sampled_test = Data_non_VEP_test.sample(n=num_vep_samples_test, random_state=987)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data_non_VEP_sampled_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vep_test, y_vep_test = create_dataset(Data_VEP_test, lookback, label=1)\n",
    "X_non_vep_test, y_non_vep_test = create_dataset(Data_non_VEP_test, lookback, label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((281, 100, 20), (281,), (21888, 100, 20), (21888,))"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_vep_test.shape, y_vep_test.shape, X_non_vep_test.shape, y_non_vep_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.concatenate((X_non_vep_test, X_vep_test), axis=0)\n",
    "y_test = np.concatenate((y_non_vep_test, y_vep_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22169, 100, 20), (22169,))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape of train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1126, 100, 20), (1126,), (22169, 100, 20), (22169,))"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(X, y):\n",
    "    indices = np.random.permutation(len(X))\n",
    "    return X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle_data(X_train, y_train)\n",
    "X_test, y_test = shuffle_data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1126, 100, 20), (1126,), (22169, 100, 20), (22169,))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting into tensors\n",
    "X_train, y_train = torch.tensor(X_train).float(), torch.tensor(y_train).long()\n",
    "X_test, y_test = torch.tensor(X_test).float(), torch.tensor(y_test).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1126, 100, 20]),\n",
       " torch.Size([1126]),\n",
       " torch.Size([22169, 100, 20]),\n",
       " torch.Size([22169]))"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EEGCNN_GRU(nn.Module):\n",
    "    def __init__(self, input_channels, sequence_length, hidden_size, num_classes):\n",
    "        super(EEGCNN_GRU, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 64, 3)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.conv2 = nn.Conv1d(64, 64, 3)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(64, 64, 3)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.maxpool = nn.MaxPool1d(2)\n",
    "        self.gru = nn.GRU(64, hidden_size, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.elu(self.bn1(self.conv1(x)))\n",
    "        x = F.elu(self.bn2(self.conv2(x)))\n",
    "        x = F.elu(self.bn3(self.conv3(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.gru(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "\n",
    "    return train_loss, train_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation loop\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels) \n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    return val_loss, val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing for subject S20.\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = EEGCNN_GRU(input_channels, sequence_length, hidden_size, num_classes).to(device)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training metrics and variables\n",
    "best_val_loss = float('inf')\n",
    "best_model_state_dict = None\n",
    "train_accuracies, val_accuracies = [], []\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "    # K-fold cross-validation within the training set\n",
    "kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n",
    "    X_train_fold, y_train_fold = X_train[train_index], y_train[train_index]\n",
    "    X_val_fold, y_val_fold = X_train[val_index], y_train[val_index]\n",
    "\n",
    "    train_loader_fold = DataLoader(TensorDataset(X_train_fold, y_train_fold), batch_size=batch_size, shuffle=True)\n",
    "    val_loader_fold = DataLoader(TensorDataset(X_val_fold, y_val_fold), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_accuracy = train(model, train_loader_fold, optimizer, criterion, device)\n",
    "        val_loss, val_accuracy = validate(model, val_loader_fold, criterion, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state_dict = model.state_dict()\n",
    "\n",
    "avg_train_accuracy = np.mean(train_accuracies)\n",
    "avg_val_accuracy = np.mean(val_accuracies)\n",
    "avg_train_loss = np.mean(train_losses)\n",
    "avg_val_loss = np.mean(val_losses)\n",
    "\n",
    "    # Plot and save training/validation accuracies and losses\n",
    "os.makedirs(f\"results_{target_subject}\", exist_ok=True)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.savefig(f\"results_{target_subject}/accuracy_curve_subject_{target_subject}.png\")\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.savefig(f\"results_{target_subject}/loss_curve_subject_{target_subject}.png\")\n",
    "plt.close()\n",
    "\n",
    "    # Load best model for testing\n",
    "model.load_state_dict(best_model_state_dict)\n",
    "torch.save(model.state_dict(), f'results_{target_subject}/{target_subject}_model.pth')\n",
    "model.eval()\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "correct, total = 0, 0\n",
    "predicted_labels, true_labels, predicted_probs_list = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_probs = F.softmax(outputs, dim=1)\n",
    "        predicted_probs_list.append(predicted_probs.cpu().numpy())\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "predicted_probs_np = np.concatenate(predicted_probs_list)\n",
    "test_accuracy = 100 * correct / total\n",
    "\n",
    "# Metrics calculations\n",
    "acc = accuracy_score(true_labels, predicted_labels)\n",
    "prec = precision_score(true_labels, predicted_labels)\n",
    "rec = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "tpr = tp / (tp + fn)\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "far = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Acceptance Rate\n",
    "frr = fn / (fn + tp) if (fn + tp) > 0 else 0  # False Rejection Rate\n",
    "\n",
    "# Calculating EER\n",
    "fpr_curve, tpr_curve, thresholds = roc_curve(true_labels, predicted_probs_np[:, 1])\n",
    "eer_threshold = thresholds[np.nanargmin(np.abs(fpr_curve - (1 - tpr_curve)))]\n",
    "eer = fpr_curve[np.nanargmin(np.abs(fpr_curve - (1 - tpr_curve)))]\n",
    "\n",
    "auc = roc_auc_score(true_labels, predicted_probs_np[:, 1])\n",
    "\n",
    "# Saving metrics as CSV\n",
    "metrics = {\n",
    "    \"avg_train_accuracy\": avg_train_accuracy,\n",
    "    \"avg_val_accuracy\": avg_val_accuracy,\n",
    "    \"avg_train_loss\": avg_train_loss,\n",
    "    \"avg_val_loss\": avg_val_loss,\n",
    "    \"test_accuracy\": test_accuracy,\n",
    "    \"precision\": prec,\n",
    "    \"recall\": rec,\n",
    "    \"f1_score\": f1,\n",
    "    \"TPR\": tpr,\n",
    "    \"FPR\": fpr,\n",
    "    \"AUC\": auc,\n",
    "    \"EER\": eer,\n",
    "    \"FAR\": far,\n",
    "    \"FRR\": frr\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "metrics_df.to_csv(f\"results_{target_subject}/metrics_subject_{target_subject}.csv\", index=False)\n",
    "\n",
    "# Saving Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, annot_kws={\"size\": 35})\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig(f\"results_{target_subject}/confusion_matrix_subject_{target_subject}.png\")\n",
    "plt.close()\n",
    "\n",
    "# Saving ROC Curve\n",
    "plt.figure()\n",
    "plt.plot(fpr_curve, tpr_curve, label=f\"ROC Curve (AUC = {auc:.2f})\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.savefig(f\"results_{target_subject}/roc_curve_subject_{target_subject}.png\")\n",
    "plt.close()\n",
    "\n",
    "print(f\"Completed processing for subject {target_subject}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    confusion_matrix, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Sample metric calculations\n",
    "acc = accuracy_score(true_labels, predicted_labels)\n",
    "prec = precision_score(true_labels, predicted_labels)\n",
    "rec = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# Confusion matrix and additional metric calculations\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # True Positive Rate\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate\n",
    "\n",
    "# False Acceptance Rate and False Rejection Rate\n",
    "far = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "frr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "\n",
    "# ROC Curve and EER calculation\n",
    "predicted_probs = predicted_probs_np[:, 1]  # Assuming second column for positive class probability\n",
    "fpr_curve, tpr_curve, thresholds = roc_curve(true_labels, predicted_probs)\n",
    "auc = roc_auc_score(true_labels, predicted_probs)\n",
    "\n",
    "# Calculate EER (Equal Error Rate)\n",
    "eer_index = np.nanargmin(np.abs(fpr_curve - (1 - tpr_curve)))\n",
    "eer = fpr_curve[eer_index]\n",
    "eer_threshold = thresholds[eer_index]\n",
    "\n",
    "# Save key metrics as CSV\n",
    "metrics = {\n",
    "    \"avg_train_accuracy\": avg_train_accuracy,\n",
    "    \"avg_val_accuracy\": avg_val_accuracy,\n",
    "    \"avg_train_loss\": avg_train_loss,\n",
    "    \"avg_val_loss\": avg_val_loss,\n",
    "    \"test_accuracy\": acc,\n",
    "    \"precision\": prec,\n",
    "    \"recall\": rec,\n",
    "    \"f1_score\": f1,\n",
    "    \"TPR\": tpr,\n",
    "    \"FPR\": fpr,\n",
    "    \"AUC\": auc,\n",
    "    \"EER\": eer,\n",
    "    \"Threshold at EER\": eer_threshold,\n",
    "    \"FAR\": far,\n",
    "    \"FRR\": frr\n",
    "}\n",
    "\n",
    "# Creating DataFrame and saving metrics to CSV\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "result_folder = f\"results_for_{target_subject}_{batch_size}\"\n",
    "os.makedirs(result_folder, exist_ok=True)\n",
    "metrics_df.to_csv(f\"{result_folder}/metrics_subject_{target_subject}.csv\", index=False)\n",
    "\n",
    "# Save FPR, TPR, and thresholds to CSV for ROC Curve\n",
    "fpr_tpr_thresh_content = \"fpr,tpr,thresholds\\n\"\n",
    "for fpr_val, tpr_val, threshold in zip(fpr_curve, tpr_curve, thresholds):\n",
    "    fpr_tpr_thresh_content += f\"{fpr_val},{tpr_val},{threshold}\\n\"\n",
    "fpr_tpr_thresh_content += f\"\\nEER:,{eer},Threshold at EER:,{eer_threshold}\\n\"\n",
    "\n",
    "# Write FPR, TPR, and threshold values along with EER to a CSV file\n",
    "with open(f\"{result_folder}/fpr_tpr_thresholds_subject_{target_subject}.csv\", \"w\") as f:\n",
    "    f.write(fpr_tpr_thresh_content)\n",
    "\n",
    "print(\"Metrics and ROC-related information saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
